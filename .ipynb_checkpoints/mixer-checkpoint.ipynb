{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Union, List\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "assert torch.cuda.is_available(), \\\n",
    "\"CUDA support is not available.\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "import LiveTune as lt\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset\n",
    "dataloader = get_dataloader(\"imagenet\", 256, model_name=\"not mixer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mixer = timm.create_model(\"mixer_b16_224.goog_in21k_ft_in1k\", pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has test accuracy=76.47%\n",
      "model has top5 accuracy=0.92%\n",
      "model has size=228.43 MiB\n",
      "model has macs=12.61 Gmacs\n",
      "average inference time is 0.0042 seconds\n",
      "model has 59.88 M parameters\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(base_mixer, dataloader=dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer blocks.11.mlp_channels\n",
      "Collapsing layer blocks.11.mlp_tokens\n",
      "Collapsing layer blocks.11.mlp_channels\n",
      "Collapsing layer blocks.11.mlp_tokens\n"
     ]
    }
   ],
   "source": [
    "collapsible_mixer = get_collapsible_model(base_mixer, fraction=.1, device=device)\n",
    "collapsible_mixer.load_state_dict(torch.load(\"./models_archive/mixer/base/new_finetuned_5epoch_frac1_lc2.pth\", map_location=device))\n",
    "collapse_model(collapsible_mixer, fraction=.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/196 [00:00<?, ?it/s]/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has test accuracy=76.11%\n",
      "model has top5 accuracy=0.92%\n",
      "model has size=212.24 MiB\n",
      "model has macs=11.71 Gmacs\n",
      "average inference time is 0.0040 seconds\n",
      "model has 55.64 M parameters\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(collapsible_mixer, dataloader=dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsible_mixer.load_state_dict(torch.load(\"./models_archive/mixer/base/finetuned_46epoch_frac02_lc.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.20399475097656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(collapsible_mixer, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.11.mlp_channels 0.9999966025352478\n",
      "blocks.11.mlp_tokens 0.999999463558197\n",
      "blocks.10.mlp_channels 0.9998729228973389\n",
      "blocks.10.mlp_tokens 0.9999915361404419\n"
     ]
    }
   ],
   "source": [
    "get_model_collapsible_slopes(collapsible_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer blocks.11.mlp_channels\n",
      "Collapsing layer blocks.11.mlp_tokens\n",
      "Collapsing layer blocks.10.mlp_channels\n",
      "Collapsing layer blocks.10.mlp_tokens\n"
     ]
    }
   ],
   "source": [
    "collapse_model(collapsible_mixer, fraction=.2, device=device, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/196 [00:00<?, ?it/s]                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.04800415039062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(collapsible_mixer, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59880472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(base_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51391800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(collapsible_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has test accuracy=74.05%\n",
      "model has top5 accuracy=0.91%\n",
      "model has size=196.04 MiB\n",
      "model has macs=10.81 Gmacs\n",
      "average inference time is 0.0057 seconds\n",
      "model has 51.39 M parameters\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(collapsible_mixer, dataloader=dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/soheil/LayerCollapse/compression_tests/mixer.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bacesipu.ucsd.edu/home/soheil/LayerCollapse/compression_tests/mixer.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model(base_mixer, dataloader\u001b[39m=\u001b[39;49mdataloader, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/LayerCollapse/compression_tests/utils.py:260\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, count_nonzero_only, device)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_model\u001b[39m(model, dataloader, count_nonzero_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 260\u001b[0m     model_test_accuracy \u001b[39m=\u001b[39m evaluate(model, dataloader[\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    261\u001b[0m     \u001b[39m# model_train_accuracy = evaluate(model, dataloader['train'], device=device)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     top5 \u001b[39m=\u001b[39m eval_top5(model, dataloader[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m], nn\u001b[39m.\u001b[39mCrossEntropyLoss(), device)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LayerCollapse/compression_tests/utils.py:137\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, verbose, device)\u001b[0m\n\u001b[1;32m    134\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    136\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    139\u001b[0m \u001b[39m# Convert logits to class indices\u001b[39;00m\n\u001b[1;32m    140\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/timm/models/mlp_mixer.py:276\u001b[0m, in \u001b[0;36mMlpMixer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 276\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    277\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    278\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/timm/models/mlp_mixer.py:261\u001b[0m, in \u001b[0;36mMlpMixer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 261\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem(x)\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    263\u001b[0m         x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(base_mixer, dataloader=dataloader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Depricated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mixer = timm.create_model(\"mixer_l16_224.goog_in21k_ft_in1k\", pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/196 [00:00<?, ?it/s]                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.33999633789062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(base_mixer, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the loaded data from mixer_async_runner.py\n",
    " Located at models_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsible_mixer_l16_224 = torch.load(\"./models_archive/mixer/large/collapsible_mixer_l16_224_5epoch_collapse0.01_frac0.5_lr0.0005.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (stem): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): MixerBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=196, out_features=384, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=384, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): CollapsibleMlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): PReLU(num_parameters=1)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapsible_mixer_l16_224.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (stem): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (18): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (19): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (20): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (21): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (22): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (23): MixerBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_tokens): Mlp(\n",
       "        (fc1): Linear(in_features=196, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=512, out_features=196, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.4739990234375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(base_mixer_b16_224, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/196 [00:00<?, ?it/s]                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.59199523925781"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(collapsible_mixer_l16_224, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all collapsible layers slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_collapsible_slopes(model, fraction=1.0):\n",
    "    num_mlp_layers = len(list(model.named_modules()))\n",
    "    for name, module in list(model.named_modules())[::-1][:int(num_mlp_layers * fraction)]:\n",
    "        if isinstance(module, CollapsibleMlp):\n",
    "            print(name, module.act.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.11.mlp_channels 0.0018727623391896486\n",
      "blocks.11.mlp_tokens 0.6893177032470703\n",
      "blocks.10.mlp_channels -0.06573034822940826\n",
      "blocks.10.mlp_tokens 0.7889662981033325\n",
      "blocks.9.mlp_channels -0.09059245884418488\n",
      "blocks.9.mlp_tokens 0.7854459881782532\n",
      "blocks.8.mlp_channels -0.10836826264858246\n",
      "blocks.8.mlp_tokens 0.7143239378929138\n",
      "blocks.7.mlp_channels -0.11396287381649017\n",
      "blocks.7.mlp_tokens 0.6591113805770874\n",
      "blocks.6.mlp_channels -0.08409617841243744\n",
      "blocks.6.mlp_tokens 0.3677079677581787\n"
     ]
    }
   ],
   "source": [
    "get_model_collapsible_slopes(collapsible_mixer_l16_224, fraction=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5005it [1:47:42,  1.29s/it]<?, ?it/s]\n",
      "100%|██████████| 1/1 [1:47:43<00:00, 6463.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.8646, Train Acc: 0.8231, Train top5: 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetune(collapsible_mixer_l16_224, lc=0, fraction=0, lr=0.0005, dataloader=dataloader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ./models_achive\n",
    "torch.save(collapsible_mixer_l16_224.state_dict(), \"./models_archive/\" + \"collapsible_mixer_l16_224_prelu_1epoch\" + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/soheil/LayerCollapse/compression_tests/mixer.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bacesipu.ucsd.edu/home/soheil/LayerCollapse/compression_tests/mixer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate(collapsible_mixer_l16_224, dataloader\u001b[39m=\u001b[39mdataloader[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(collapsible_mixer_l16_224, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "finetune(collapsible_mixer_l16_224, lc=0, fraction=0, lr=0.0005, dataloader=dataloader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(collapsible_mixer_l16_224, dataloader=dataloader['val'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/soheil/LayerCollapse/compression_tests/mixer.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bacesipu.ucsd.edu/home/soheil/LayerCollapse/compression_tests/mixer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate_model(base_mixer_l16_224, dataloader, count_nonzero_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/LayerCollapse/compression_tests/utils.py:211\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, count_nonzero_only, device)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_model\u001b[39m(model, dataloader, count_nonzero_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 211\u001b[0m     model_test_accuracy \u001b[39m=\u001b[39m evaluate(model, dataloader[\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    212\u001b[0m     model_train_accuracy \u001b[39m=\u001b[39m evaluate(model, dataloader[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LayerCollapse/compression_tests/utils.py:126\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, verbose, device)\u001b[0m\n\u001b[1;32m    123\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    125\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    128\u001b[0m \u001b[39m# Convert logits to class indices\u001b[39;00m\n\u001b[1;32m    129\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/timm/models/mlp_mixer.py:276\u001b[0m, in \u001b[0;36mMlpMixer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 276\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    277\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    278\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/timm/models/mlp_mixer.py:261\u001b[0m, in \u001b[0;36mMlpMixer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 261\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem(x)\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    263\u001b[0m         x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/timm/layers/patch_embed.py:87\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     pad_w \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m W \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[\u001b[39m1\u001b[39m]) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[\u001b[39m1\u001b[39m]\n\u001b[1;32m     86\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(x, (\u001b[39m0\u001b[39m, pad_w, \u001b[39m0\u001b[39m, pad_h))\n\u001b[0;32m---> 87\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x)\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten:\n\u001b[1;32m     89\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NCHW -> NLC\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "evaluate_model(base_mixer_l16_224, dataloader, count_nonzero_only=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
