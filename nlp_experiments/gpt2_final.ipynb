{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from clm_utils import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"distilgpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "lm_datasets, data_collator, encodings_for_eval_pt = get_wikitext_dataset(tokenizer, block_size=128, fraction_of_train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/distilgpt2/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings_for_eval_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_perplexity(model_base, \u001b[43mencodings_for_eval_pt\u001b[49m, \u001b[38;5;241m32\u001b[39m, device, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encodings_for_eval_pt' is not defined"
     ]
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.5.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/distilgpt2/LC_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformer.h.5.mlp: loss:  tensor([0.0009], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "get_layer_gates_loss(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4493/4495 [00:34<00:00, 130.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.9670467376709"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 64, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.5.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 287516/287644 [36:15<00:00, 132.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.539182662963867"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 1, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC: 77780736\n",
      "Number of parameters in the model base: 81912576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC: {get_num_parameters(model_LC)}\")\n",
    "print(f\"Number of parameters in the model base: {get_num_parameters(model_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 141.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.870046615600586"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model base: 124439808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model base: {get_num_parameters(model_base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformer.h.11.mlp: loss:  tensor([0.0004], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "get_layer_gates_loss(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 140.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.467987060546875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC: 120307968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC: {get_num_parameters(model_LC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.6.mlp\n"
     ]
    }
   ],
   "source": [
    "model_GP = get_LC_model_gpt2(model_base, num_GP_layers=1, blacklist=[\"transformer.h.11.mlp\", \"transformer.h.10.mlp\", \"transformer.h.9.mlp\", \"transformer.h.8.mlp\", \"transformer.h.7.mlp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GP.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_middle.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.6.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_GP)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 142.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.09252166748047"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_GP, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "model_GP = get_LC_model_gpt2(model_base, num_GP_layers=1, only_list=[\"10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GP.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_middle_layer10.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 142.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.230045318603516"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_GP, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.11.mlp\n",
      "Setting up gated layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_2layer.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:04<00:00, 140.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.066349029541016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.11.mlp\n",
      "Collapsing layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:01<00:00, 145.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.086347579956055"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC 2 layers: 116176128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC 2 layers: {get_num_parameters(model_LC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2-large\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2-large/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [03:16<00:00, 45.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.6498966217041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774030080"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.32.mlp\n",
      "Setting up gated layer transformer.h.16.mlp\n",
      "Setting up gated layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1, only_list=[\"32\", \"16\", \"11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2-large/LC_adam_no_drop_middle.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.32.mlp\n",
      "Collapsing layer transformer.h.16.mlp\n",
      "Collapsing layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [03:08<00:00, 47.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.175317764282227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739608320"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_LC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.35.mlp\n",
      "Setting up gated layer transformer.h.34.mlp\n",
      "Setting up gated layer transformer.h.33.mlp\n",
      "Setting up gated layer transformer.h.32.mlp\n",
      "Setting up gated layer transformer.h.31.mlp\n",
      "Setting up gated layer transformer.h.30.mlp\n",
      "Setting up gated layer transformer.h.29.mlp\n",
      "Setting up gated layer transformer.h.28.mlp\n",
      "Setting up gated layer transformer.h.27.mlp\n",
      "Setting up gated layer transformer.h.26.mlp\n",
      "Setting up gated layer transformer.h.25.mlp\n",
      "Setting up gated layer transformer.h.24.mlp\n",
      "Setting up gated layer transformer.h.23.mlp\n",
      "Setting up gated layer transformer.h.22.mlp\n",
      "Setting up gated layer transformer.h.21.mlp\n",
      "Setting up gated layer transformer.h.20.mlp\n",
      "Setting up gated layer transformer.h.19.mlp\n",
      "Setting up gated layer transformer.h.18.mlp\n",
      "Setting up gated layer transformer.h.17.mlp\n",
      "Setting up gated layer transformer.h.16.mlp\n",
      "Setting up gated layer transformer.h.15.mlp\n",
      "Setting up gated layer transformer.h.14.mlp\n",
      "Setting up gated layer transformer.h.13.mlp\n",
      "Setting up gated layer transformer.h.12.mlp\n",
      "Setting up gated layer transformer.h.11.mlp\n",
      "Setting up gated layer transformer.h.10.mlp\n",
      "Setting up gated layer transformer.h.9.mlp\n",
      "Setting up gated layer transformer.h.8.mlp\n",
      "Setting up gated layer transformer.h.7.mlp\n",
      "Setting up gated layer transformer.h.6.mlp\n",
      "Setting up gated layer transformer.h.5.mlp\n",
      "Setting up gated layer transformer.h.4.mlp\n",
      "Setting up gated layer transformer.h.3.mlp\n",
      "Setting up gated layer transformer.h.2.mlp\n",
      "Setting up gated layer transformer.h.1.mlp\n",
      "Setting up gated layer transformer.h.0.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.35.mlp\n",
      "Collapsing layer transformer.h.34.mlp\n",
      "Collapsing layer transformer.h.33.mlp\n",
      "Collapsing layer transformer.h.32.mlp\n",
      "Collapsing layer transformer.h.31.mlp\n",
      "Collapsing layer transformer.h.30.mlp\n",
      "Collapsing layer transformer.h.29.mlp\n",
      "Collapsing layer transformer.h.28.mlp\n",
      "Collapsing layer transformer.h.27.mlp\n",
      "Collapsing layer transformer.h.26.mlp\n",
      "Collapsing layer transformer.h.25.mlp\n",
      "Collapsing layer transformer.h.24.mlp\n",
      "Collapsing layer transformer.h.23.mlp\n",
      "Collapsing layer transformer.h.22.mlp\n",
      "Collapsing layer transformer.h.21.mlp\n",
      "Collapsing layer transformer.h.20.mlp\n",
      "Collapsing layer transformer.h.19.mlp\n",
      "Collapsing layer transformer.h.18.mlp\n",
      "Collapsing layer transformer.h.17.mlp\n",
      "Collapsing layer transformer.h.16.mlp\n",
      "Collapsing layer transformer.h.15.mlp\n",
      "Collapsing layer transformer.h.14.mlp\n",
      "Collapsing layer transformer.h.13.mlp\n",
      "Collapsing layer transformer.h.12.mlp\n",
      "Collapsing layer transformer.h.11.mlp\n",
      "Collapsing layer transformer.h.10.mlp\n",
      "Collapsing layer transformer.h.9.mlp\n",
      "Collapsing layer transformer.h.8.mlp\n",
      "Collapsing layer transformer.h.7.mlp\n",
      "Collapsing layer transformer.h.6.mlp\n",
      "Collapsing layer transformer.h.5.mlp\n",
      "Collapsing layer transformer.h.4.mlp\n",
      "Collapsing layer transformer.h.3.mlp\n",
      "Collapsing layer transformer.h.2.mlp\n",
      "Collapsing layer transformer.h.1.mlp\n",
      "Collapsing layer transformer.h.0.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360968960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.533649958409885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(774030080 - 360968960)/774030080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.ones((1, 128), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47504752640"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_LC.to(device), dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100447354880"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_base.to(device), dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.527068157277493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100447354880 - 47504752640) / 100447354880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
