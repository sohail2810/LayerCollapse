{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from clm_utils import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"distilgpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "lm_datasets, data_collator, encodings_for_eval_pt = get_wikitext_dataset(tokenizer, block_size=128, fraction_of_train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/distilgpt2/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encodings_for_eval_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_perplexity(model_base, \u001b[43mencodings_for_eval_pt\u001b[49m, \u001b[38;5;241m32\u001b[39m, device, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encodings_for_eval_pt' is not defined"
     ]
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.5.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/distilgpt2/LC_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformer.h.5.mlp: loss:  tensor([0.0009], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "get_layer_gates_loss(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4493/4495 [00:34<00:00, 130.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.9670467376709"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 64, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.5.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 287516/287644 [36:15<00:00, 132.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.539182662963867"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 1, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC: 77780736\n",
      "Number of parameters in the model base: 81912576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC: {get_num_parameters(model_LC)}\")\n",
    "print(f\"Number of parameters in the model base: {get_num_parameters(model_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:06<00:00, 134.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.870046615600586"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model base: 124439808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model base: {get_num_parameters(model_base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformer.h.11.mlp: loss:  tensor([0.0004], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "get_layer_gates_loss(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 140.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.467987060546875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC: 120307968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC: {get_num_parameters(model_LC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/vanilla_adam.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [02:28<00:00, 60.56it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.90892219543457"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned_base = prune_l1_model(model_base, amount=0.1)\n",
    "model_pruned_base = remove_prune(model_pruned_base).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [02:19<00:00, 64.45it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.964061737060547"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_pruned_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(115946340, device='cuda:1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_pruned_base, count_nonzero_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.ones((1, 128), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16135421952"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_pruned_base.to(\"cpu\"), dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16135421952"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_base.to(\"cpu\"), dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.6.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1, only_list=[\"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_middle.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.6.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 142.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.09252166748047"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "model_GP = get_LC_model_gpt2(model_base, num_GP_layers=1, only_list=[\"10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GP.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_middle_layer10.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 142.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.230045318603516"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_GP, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.11.mlp\n",
      "Setting up gated layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2/LC_adam_no_drop_2layer.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:04<00:00, 140.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.066349029541016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.11.mlp\n",
      "Collapsing layer transformer.h.10.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:01<00:00, 145.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.086347579956055"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model LC 2 layers: 116176128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters in the model LC 2 layers: {get_num_parameters(model_LC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.6\n"
     ]
    }
   ],
   "source": [
    "model_GP = get_GP_model_gpt2(model_base, num_GP_layers=1, only_list=[\"6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GP.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2-GP/gpt2/LC_adam_no_drop_layer6.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.6\n"
     ]
    }
   ],
   "source": [
    "collapse_GP_model(model_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 141.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.295211791992188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_GP, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119715841"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pytorch_utils import Conv1D\n",
    "def prune_l1_model(model, amount):\n",
    "    copy_model = copy.deepcopy(model)\n",
    "    for name, module in copy_model.named_modules():\n",
    "        if isinstance(module, Conv1D):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return copy_model\n",
    "def remove_prune(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, Conv1D):\n",
    "            prune.remove(module, name='weight')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned_base = prune_l1_model(model_base, amount=0.2)\n",
    "model_pruned_base = remove_prune(model_pruned_base).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(107452884, device='cuda:1')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_pruned_base, count_nonzero_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [02:13<00:00, 67.20it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.17877197265625"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_pruned_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LC_pruned = prune_l1_model(model_LC, amount=0.2)\n",
    "model_LC_pruned = remove_prune(model_LC_pruned).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(104146797, device='cuda:1')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_LC_pruned, count_nonzero_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [01:03<00:00, 141.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.40266227722168"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC_pruned, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base, tokenizer = get_classification_gpt2_model(pre_trained_model_name=\"gpt2-large\", embd_pdrop=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2-large/vanilla_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [03:16<00:00, 45.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.6498966217041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774030080"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.32.mlp\n",
      "Setting up gated layer transformer.h.16.mlp\n",
      "Setting up gated layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=1, only_list=[\"32\", \"16\", \"11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LC.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2/gpt2-large/LC_adam_no_drop_middle.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.32.mlp\n",
      "Collapsing layer transformer.h.16.mlp\n",
      "Collapsing layer transformer.h.11.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [03:08<00:00, 47.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.175317764282227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_LC, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739608320"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_LC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.32\n",
      "Setting up gated layer transformer.h.16\n",
      "Setting up gated layer transformer.h.11\n"
     ]
    }
   ],
   "source": [
    "model_GP = get_GP_model_gpt2(model_base, num_GP_layers=3, only_list=[\"32\", \"16\", \"11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GP.load_state_dict(torch.load(\"/scr/models/LC/models_archive/GPT2-GP/gpt2-large/LC_adam_no_drop.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.32\n",
      "Collapsing layer transformer.h.16\n",
      "Collapsing layer transformer.h.11\n"
     ]
    }
   ],
   "source": [
    "collapse_GP_model(model_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [06:38<00:00, 22.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.615042686462402"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_GP, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734681603"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pruned_base = prune_l1_model(model_base, amount=0.05)\n",
    "model_pruned_base = remove_prune(model_pruned_base).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(738640640, device='cuda:1')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_pruned_base, count_nonzero_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8985/8989 [03:17<00:00, 45.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.650970458984375"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(model_pruned_base, encodings_for_eval_pt, 32, device, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gated layer transformer.h.35.mlp\n",
      "Setting up gated layer transformer.h.34.mlp\n",
      "Setting up gated layer transformer.h.33.mlp\n",
      "Setting up gated layer transformer.h.32.mlp\n",
      "Setting up gated layer transformer.h.31.mlp\n",
      "Setting up gated layer transformer.h.30.mlp\n",
      "Setting up gated layer transformer.h.29.mlp\n",
      "Setting up gated layer transformer.h.28.mlp\n",
      "Setting up gated layer transformer.h.27.mlp\n",
      "Setting up gated layer transformer.h.26.mlp\n",
      "Setting up gated layer transformer.h.25.mlp\n",
      "Setting up gated layer transformer.h.24.mlp\n",
      "Setting up gated layer transformer.h.23.mlp\n",
      "Setting up gated layer transformer.h.22.mlp\n",
      "Setting up gated layer transformer.h.21.mlp\n",
      "Setting up gated layer transformer.h.20.mlp\n",
      "Setting up gated layer transformer.h.19.mlp\n",
      "Setting up gated layer transformer.h.18.mlp\n",
      "Setting up gated layer transformer.h.17.mlp\n",
      "Setting up gated layer transformer.h.16.mlp\n",
      "Setting up gated layer transformer.h.15.mlp\n",
      "Setting up gated layer transformer.h.14.mlp\n",
      "Setting up gated layer transformer.h.13.mlp\n",
      "Setting up gated layer transformer.h.12.mlp\n",
      "Setting up gated layer transformer.h.11.mlp\n",
      "Setting up gated layer transformer.h.10.mlp\n",
      "Setting up gated layer transformer.h.9.mlp\n",
      "Setting up gated layer transformer.h.8.mlp\n",
      "Setting up gated layer transformer.h.7.mlp\n",
      "Setting up gated layer transformer.h.6.mlp\n",
      "Setting up gated layer transformer.h.5.mlp\n",
      "Setting up gated layer transformer.h.4.mlp\n",
      "Setting up gated layer transformer.h.3.mlp\n",
      "Setting up gated layer transformer.h.2.mlp\n",
      "Setting up gated layer transformer.h.1.mlp\n",
      "Setting up gated layer transformer.h.0.mlp\n"
     ]
    }
   ],
   "source": [
    "model_LC = get_LC_model_gpt2(model_base, num_GP_layers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing layer transformer.h.35.mlp\n",
      "Collapsing layer transformer.h.34.mlp\n",
      "Collapsing layer transformer.h.33.mlp\n",
      "Collapsing layer transformer.h.32.mlp\n",
      "Collapsing layer transformer.h.31.mlp\n",
      "Collapsing layer transformer.h.30.mlp\n",
      "Collapsing layer transformer.h.29.mlp\n",
      "Collapsing layer transformer.h.28.mlp\n",
      "Collapsing layer transformer.h.27.mlp\n",
      "Collapsing layer transformer.h.26.mlp\n",
      "Collapsing layer transformer.h.25.mlp\n",
      "Collapsing layer transformer.h.24.mlp\n",
      "Collapsing layer transformer.h.23.mlp\n",
      "Collapsing layer transformer.h.22.mlp\n",
      "Collapsing layer transformer.h.21.mlp\n",
      "Collapsing layer transformer.h.20.mlp\n",
      "Collapsing layer transformer.h.19.mlp\n",
      "Collapsing layer transformer.h.18.mlp\n",
      "Collapsing layer transformer.h.17.mlp\n",
      "Collapsing layer transformer.h.16.mlp\n",
      "Collapsing layer transformer.h.15.mlp\n",
      "Collapsing layer transformer.h.14.mlp\n",
      "Collapsing layer transformer.h.13.mlp\n",
      "Collapsing layer transformer.h.12.mlp\n",
      "Collapsing layer transformer.h.11.mlp\n",
      "Collapsing layer transformer.h.10.mlp\n",
      "Collapsing layer transformer.h.9.mlp\n",
      "Collapsing layer transformer.h.8.mlp\n",
      "Collapsing layer transformer.h.7.mlp\n",
      "Collapsing layer transformer.h.6.mlp\n",
      "Collapsing layer transformer.h.5.mlp\n",
      "Collapsing layer transformer.h.4.mlp\n",
      "Collapsing layer transformer.h.3.mlp\n",
      "Collapsing layer transformer.h.2.mlp\n",
      "Collapsing layer transformer.h.1.mlp\n",
      "Collapsing layer transformer.h.0.mlp\n"
     ]
    }
   ],
   "source": [
    "collapse_model(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360968960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(model_LC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.533649958409885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(774030080 - 360968960)/774030080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.ones((1, 128), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47504752640"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_LC.to(device), dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::full\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/soheil/miniconda3/envs/GP/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::where\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100447354880"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_macs(model_base.to(device), dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.527068157277493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100447354880 - 47504752640) / 100447354880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
